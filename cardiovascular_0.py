# -*- coding: utf-8 -*-
"""CardioVascular_0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WhoVU1i03GVeHMU8law2DzxV6fNr0ATr

#Sprint 1

##Import Necessory Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier,plot_tree
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score,confusion_matrix

#Dataset Reading
dataset=pd.read_csv('cardio_train.csv',sep=';')
dataset

"""*   Dataset Contain 12 attributes + target variable
*  70000 details of patients

After remove the ID column the the dataset contain 12 features
"""

dataset.head()

#Dataset Shape
dataset.shape

"""70000 rows, 13 columns"""

#Dataset information
dataset.info()

"""Dataset contain 12 integers and 1 decimal values( Weight)"""

dataset.dtypes

#Dataset Description
dataset.describe()

"""Age feature contain numerical value in Days"""

dataset.isna()

#Check null values
dataset.isnull().sum()

"""The dataset does not contain any null values."""

#Check Duplicate Values
dataset.duplicated().sum()

"""The dataset does not contains any duplicate values."""

#convert ages from days to years
dataset["age"]=(dataset["age"]/365).values.astype(int)
dataset.head()

dataset.describe()

dataset['cardio'].value_counts()

"""##Dataset visualization"""

dataset.hist(figsize=(15,12))

dataset['cardio'].value_counts()

fig, ax = plt.subplots(1, 2, figsize=(20, 7))

sns.countplot(data=dataset, x="cardio", ax=ax[0])
dataset["cardio"].value_counts().plot.pie(explode=[0.1, 0],autopct="%1.1f%%", labels=["0", "1"], shadow=True, ax=ax[1])

plt.show()

from matplotlib import rcParams
rcParams['figure.figsize'] = 12, 8
sns.countplot(x='age', hue='cardio', data = dataset, palette="Set2");

"""We can see that around 55 age over the people have CVD disease"""

df_categorical = dataset.loc[:,['cholesterol','gluc', 'smoke', 'alco', 'active']]
sns.countplot(x="variable", hue="value", data=pd.melt(df_categorical), palette="Set2");

"""Cholestrol
1.   Normal - 52385 patients
2.   Above normal - 9549 patients
3.   Well above normal - 8066 patients

Glucose level
1.   Normal - 59479
2.   Above normal - 5331
3.   Well above normal - 5190

Smoke
1.   Yes - 6169
2.   No - 63831

Alcoholic
1.   Yes - 3764
2.   No - 66236

Physicaly active

1.   Yes - 56261
2.   No - 13739


















"""

cholesterol_counts = dataset['cholesterol'].value_counts()

# Display the counts
print("Counts for 'cholesterol' column:")
print(cholesterol_counts)

glucose_counts = dataset['gluc'].value_counts()

# Display the counts
print("Counts for 'gluc' column:")
print(glucose_counts)

smoke_counts = dataset['smoke'].value_counts()

# Display the counts
print("Counts for 'smoke' column:")
print(smoke_counts)

alco_counts = dataset['alco'].value_counts()

# Display the counts
print("Counts for 'alco' column:")
print(alco_counts)

active_counts = dataset['active'].value_counts()

# Display the counts
print("Counts for 'active' column:")
print(active_counts)

dataset['gender'].value_counts()

"""The dataset contain 45530 womens and 24470 mens."""

plt.figure(figsize=(8, 6))
dataset.groupby('gender')['cardio'].mean().plot(kind='bar', color=['blue', 'pink'])
plt.title('Risk of Cardiovascular Disease by Gender')
plt.xlabel('Gender')
plt.ylabel('Risk (Proportion with Cardiovascular Disease)')
plt.xticks(rotation=0)
plt.show()

df_long = pd.melt(dataset, id_vars=['cardio'], value_vars=['cholesterol','gluc', 'smoke', 'alco', 'active'])
sns.catplot(x="variable", hue="value", col="cardio",
                data=df_long, kind="count");

"""

1.   We can see that patients with CVD have high cholestrol and blood glucose level.
2.   Comparatively, they less active.

"""

dataset.boxplot(figsize=(14,6))

dataset[["age","height","weight","ap_hi","ap_lo"]].boxplot()

"""We can see that ap_hi(Systolic Blood pressure) and ap_hi(Diastolic Blood pressure) have outliers.

##Dataset preprocessing
"""

#Remove Id
dataset.drop("id",axis=1,inplace=True)
dataset.head()

import pandas as pd

# Assuming you have a DataFrame called 'df' with columns 'ap_hi' and 'ap_lo'

# Calculate the IQR for ap_hi and ap_lo
Q1_hi = dataset['ap_hi'].quantile(0.25)
Q3_hi = dataset['ap_hi'].quantile(0.75)
IQR_hi = Q3_hi - Q1_hi

Q1_lo = dataset['ap_lo'].quantile(0.25)
Q3_lo = dataset['ap_lo'].quantile(0.75)
IQR_lo = Q3_lo - Q1_lo

# Define the upper and lower bounds for outliers
upper_bound_hi = Q3_hi + 1.5 * IQR_hi
lower_bound_hi = Q1_hi - 1.5 * IQR_hi

upper_bound_lo = Q3_lo + 1.5 * IQR_lo
lower_bound_lo = Q1_lo - 1.5 * IQR_lo

# Identify and filter out outliers
outliers_hi = (dataset['ap_hi'] > upper_bound_hi) | (dataset['ap_hi'] < lower_bound_hi)
outliers_lo = (dataset['ap_lo'] > upper_bound_lo) | (dataset['ap_lo'] < lower_bound_lo)

# Replace outliers with the median (you can choose a different strategy based on your analysis)
dataset.loc[outliers_hi, 'ap_hi'] = dataset['ap_hi'].median()
dataset.loc[outliers_lo, 'ap_lo'] = dataset['ap_lo'].median()

dataset.boxplot(figsize=(14,6))

dataset.describe()

# Calculate the IQR for age, height, and weight
Q1_age = dataset['age'].quantile(0.25)
Q3_age = dataset['age'].quantile(0.75)
IQR_age = Q3_age - Q1_age

Q1_height = dataset['height'].quantile(0.25)
Q3_height = dataset['height'].quantile(0.75)
IQR_height = Q3_height - Q1_height

Q1_weight = dataset['weight'].quantile(0.25)
Q3_weight = dataset['weight'].quantile(0.75)
IQR_weight = Q3_weight - Q1_weight

# Define the upper and lower bounds for outliers
upper_bound_age = Q3_age + 1.5 * IQR_age
lower_bound_age = Q1_age - 1.5 * IQR_age

upper_bound_height = Q3_height + 1.5 * IQR_height
lower_bound_height = Q1_height - 1.5 * IQR_height

upper_bound_weight = Q3_weight + 1.5 * IQR_weight
lower_bound_weight = Q1_weight - 1.5 * IQR_weight

# Identify and filter out outliers for age, height, and weight
outliers_age = (dataset['age'] > upper_bound_age) | (dataset['age'] < lower_bound_age)
outliers_height = (dataset['height'] > upper_bound_height) | (dataset['height'] < lower_bound_height)
outliers_weight = (dataset['weight'] > upper_bound_weight) | (dataset['weight'] < lower_bound_weight)

# Replace outliers with the median (you can choose a different strategy based on your analysis)
dataset.loc[outliers_age, 'age'] = dataset['age'].median()
dataset.loc[outliers_height, 'height'] = dataset['height'].median()
dataset.loc[outliers_weight, 'weight'] = dataset['weight'].median()

# Calculate the IQR for gluc, smoke, alco, and active
Q1_gluc = dataset['gluc'].quantile(0.25)
Q3_gluc = dataset['gluc'].quantile(0.75)
IQR_gluc = Q3_gluc - Q1_gluc

Q1_smoke = dataset['smoke'].quantile(0.25)
Q3_smoke = dataset['smoke'].quantile(0.75)
IQR_smoke = Q3_smoke - Q1_smoke

Q1_alco = dataset['alco'].quantile(0.25)
Q3_alco = dataset['alco'].quantile(0.75)
IQR_alco = Q3_alco - Q1_alco

Q1_active = dataset['active'].quantile(0.25)
Q3_active = dataset['active'].quantile(0.75)
IQR_active = Q3_active - Q1_active

# Define the upper and lower bounds for outliers
upper_bound_gluc = Q3_gluc + 1.5 * IQR_gluc
lower_bound_gluc = Q1_gluc - 1.5 * IQR_gluc

upper_bound_smoke = Q3_smoke + 1.5 * IQR_smoke
lower_bound_smoke = Q1_smoke - 1.5 * IQR_smoke

upper_bound_alco = Q3_alco + 1.5 * IQR_alco
lower_bound_alco = Q1_alco - 1.5 * IQR_alco

upper_bound_active = Q3_active + 1.5 * IQR_active
lower_bound_active = Q1_active - 1.5 * IQR_active

# Identify and filter out outliers for gluc, smoke, alco, and active
outliers_gluc = (dataset['gluc'] > upper_bound_gluc) | (dataset['gluc'] < lower_bound_gluc)
outliers_smoke = (dataset['smoke'] > upper_bound_smoke) | (dataset['smoke'] < lower_bound_smoke)
outliers_alco = (dataset['alco'] > upper_bound_alco) | (dataset['alco'] < lower_bound_alco)
outliers_active = (dataset['active'] > upper_bound_active) | (dataset['active'] < lower_bound_active)

# Replace outliers with the median (you can choose a different strategy based on your analysis)
dataset.loc[outliers_gluc, 'gluc'] = dataset['gluc'].median()
dataset.loc[outliers_smoke, 'smoke'] = dataset['smoke'].median()
dataset.loc[outliers_alco, 'alco'] = dataset['alco'].median()
dataset.loc[outliers_active, 'active'] = dataset['active'].median()

# Calculate the IQR for weight and bmi
Q1_weight = dataset['weight'].quantile(0.25)
Q3_weight = dataset['weight'].quantile(0.75)
IQR_weight = Q3_weight - Q1_weight


# Define the upper and lower bounds for outliers
upper_bound_weight = Q3_weight + 1.5 * IQR_weight
lower_bound_weight = Q1_weight - 1.5 * IQR_weight


# Identify and filter out outliers for weight and bmi
outliers_weight = (dataset['weight'] > upper_bound_weight) | (dataset['weight'] < lower_bound_weight)


# Replace outliers with the median (you can choose a different strategy based on your analysis)
dataset.loc[outliers_weight, 'weight'] = dataset['weight'].median()

dataset.boxplot(figsize=(14,6))

dataset.describe()

dataset["bmi"] = np.round(dataset.weight/(dataset.height/100)**2,2)
print(dataset.shape)
dataset.head(3)

plt.figure(figsize=(8, 6))
dataset.groupby('gender')['cardio'].mean().plot(kind='bar', color=['blue', 'pink'])
plt.title('Risk of Cardiovascular Disease by Gender')
plt.xlabel('Gender')
plt.ylabel('Risk (Proportion with Cardiovascular Disease)')
plt.xticks(rotation=0)
plt.show()

dataset["gender"].value_counts()

import pandas as pd
from imblearn.over_sampling import RandomOverSampler

# Assuming 'dataset' is your cardiovascular disease dataset DataFrame
# Separate features (x) and target variable (y)
xx = dataset.drop('gender', axis=1)
yy = dataset['gender']

# Determine the desired number of samples for each class in the 'gender' column
desired_samples = yy.value_counts().max()  # Adjust this as needed

# Define the sampling strategy for RandomOverSampler
sampling_strategy = {1: desired_samples, 2: desired_samples}

# Apply random oversampling to the entire dataset for the 'gender' column
ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)
X_resampled, y_resampled = ros.fit_resample(xx, yy)

# Combine the resampled data back into a DataFrame
df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=xx.columns), pd.DataFrame(y_resampled, columns=['gender'])], axis=1)

# Display the resampled 'gender' column counts
print(df_resampled['gender'].value_counts())

plt.figure(figsize=(8, 6))
dataset.groupby('gender')['cardio'].mean().plot(kind='bar', color=['blue', 'pink'])
plt.title('Risk of Cardiovascular Disease by Gender')
plt.xlabel('Gender')
plt.ylabel('Risk (Proportion with Cardiovascular Disease)')
plt.xticks(rotation=0)
plt.show()

df_resampled['cardio'].value_counts()

df_resampled

"""##split the dataset"""

x=df_resampled.drop('cardio',axis=1)
y=df_resampled['cardio']
x_train,x_test,y_train,y_test=train_test_split(x, y,test_size=0.2,random_state=42)

x_train



y_train

"""##Train the models and calculate the acurracy

##Model - Decision Tree
"""

#Train decision tree
dcl=DecisionTreeClassifier()
dcl.fit(x_train,y_train)

predict=dcl.predict(x_test)
DecisionTree_accuracy=accuracy_score(y_test,predict)
print(f"Accuracy:{DecisionTree_accuracy:.2f}")
print("Decision Tree Classification Report:\n", classification_report(y_test, predict))

train_predict = dcl.predict(x_train)
dcl_train_accura = accuracy_score(y_train,train_predict)
print(dcl_train_accura)

cm=confusion_matrix(y_test,predict)
print("Confussion Matrix")
print(cm)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the confusion matrix
cm = confusion_matrix(y_test, predict)

# Plotting confusion matrix using seaborn
plt.figure(figsize=(3, 2))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

df_resampled

import pickle

# Assuming dt_scratch is your trained decision tree model

# Save the model to a file
with open('decision_tree.pkl_2', 'wb') as file:
    pickle.dump(dt_scratch, file)

df_resampled=['age','gender','height','weight','bmi','ap_hi','ap_lo','cholestrol','gluc','smoke','alco','active']

# Assuming you have a trained Decision Tree model (dcl)

# Get user input for features
age = float(input("Enter age: "))
height = int(input("Enter height in cm: "))
weight = float(input("Enter weight in kg: "))
ap_hi = int(input("Enter systolic blood pressure (ap_hi): "))
ap_lo = int(input("Enter diastolic blood pressure (ap_lo): "))
cholesterol = int(input("Enter cholesterol level: "))
gluc = int(input("Enter glucose level: "))
smoke = int(input("Enter smoking status (0 for non-smoker, 1 for smoker): "))
alco = int(input("Enter alcohol consumption status (0 for non-drinker, 1 for drinker): "))
active = int(input("Enter physical activity status (0 for inactive, 1 for active): "))
bmi = float(input("Enter BMI: "))
gender = int(input("Enter gender (1 for male, 2 for female): "))

# Create a DataFrame with user input
new_data = pd.DataFrame({
    'age': [age],
    'height': [height],
    'weight': [weight],
    'ap_hi': [ap_hi],
    'ap_lo': [ap_lo],
    'cholesterol': [cholesterol],
    'gluc': [gluc],
    'smoke': [smoke],
    'alco': [alco],
    'active': [active],
    'bmi': [bmi],
    'gender': [gender],

})

# Make predictions
predictions = dcl.predict(new_data)

# Display the predictions
print("Predictions:", predictions)

import numpy as np

class DecisionTree:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth
        self.tree = None

    def entropy(self, y):
        classes, counts = np.unique(y, return_counts=True)
        probabilities = counts / len(y)
        entropy = -np.sum(probabilities * np.log2(probabilities))
        return entropy

    def information_gain(self, X, y, feature, threshold):
        mask = X[:, feature] <= threshold
        left_y = y[mask]
        right_y = y[~mask]

        left_entropy = self.entropy(left_y)
        right_entropy = self.entropy(right_y)

        total_entropy = len(left_y) / len(y) * left_entropy + len(right_y) / len(y) * right_entropy

        information_gain = self.entropy(y) - total_entropy
        return information_gain

    def find_best_split(self, X, y):
        num_features = X.shape[1]
        best_feature = None
        best_threshold = None
        best_gain = -1

        for feature in range(num_features):
            thresholds = np.unique(X[:, feature])
            for threshold in thresholds:
                gain = self.information_gain(X, y, feature, threshold)
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature
                    best_threshold = threshold

        return best_feature, best_threshold

    def build_tree(self, X, y, depth=0):
        if depth == self.max_depth or len(np.unique(y)) == 1:
            return np.bincount(y).argmax()

        best_feature, best_threshold = self.find_best_split(X, y)

        if best_feature is not None:
            mask = X[:, best_feature] <= best_threshold
            left_subtree = self.build_tree(X[mask], y[mask], depth + 1)
            right_subtree = self.build_tree(X[~mask], y[~mask], depth + 1)
            return (best_feature, best_threshold, left_subtree, right_subtree)
        else:
            return np.bincount(y).argmax()

    def fit(self, X, y):
        self.tree = self.build_tree(X, y)

    def predict_instance(self, x, tree):
        if isinstance(tree, np.int64):
            return tree  # leaf node

        feature, threshold, left_subtree, right_subtree = tree

        if x[feature] <= threshold:
            return self.predict_instance(x, left_subtree)
        else:
            return self.predict_instance(x, right_subtree)

    def predict(self, X):
        predictions = []
        for instance in X:
            prediction = self.predict_instance(instance, self.tree)
            predictions.append(prediction)
        return np.array(predictions)

# Assuming x_train and y_train are your training features and labels
# You may need to convert categorical features into numerical format for this simple implementation

# Create and train the decision tree
dt_scratch = DecisionTree(max_depth=5)
dt_scratch.fit(x_train.values, y_train.values)

# Make predictions on the test set
predictions_scratch = dt_scratch.predict(x_test.values)

# Evaluate the accuracy of the model
accuracy_scratch = accuracy_score(y_test.values, predictions_scratch)
print(f"Accuracy (Decision Tree from Scratch): {accuracy_scratch:.2f}")

"""##Model - Logistic Regression"""

# Train the model Logistic Regression

logreg = LogisticRegression(max_iter=1000)
logreg.fit(x_train,y_train)

logreg_prediction = logreg.predict(x_test)
acc_log = accuracy_score(y_test,logreg_prediction)
print(f"Accuracy:{acc_log:.2f}")
print("Logistic Regression Classification Report:\n", classification_report(y_test, logreg_prediction))

log_train_predict = logreg.predict(x_train)
log_train_accu = accuracy_score(y_train,log_train_predict)
print(log_train_accu)

cm=confusion_matrix(y_test,logreg_prediction)
print("Confussion Matrix")
print(cm)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the confusion matrix
cm = confusion_matrix(y_test, logreg_prediction)

# Plotting confusion matrix using seaborn
plt.figure(figsize=(3, 2))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""##Model - KNN"""

# Train the model k-Nearest Neighbors

knn_model = KNeighborsClassifier()
knn_model.fit(x_train, y_train)

knn_predictions = knn_model.predict(x_test)
knn_accuracy = accuracy_score(y_test, knn_predictions)
print(f"\nk-Nearest Neighbors (KNN) Accuracy:{knn_accuracy:.2f}")
print("k-Nearest Neighbors (KNN) Classification Report:\n", classification_report(y_test, knn_predictions))

knn_train_pred = knn_model.predict(x_train)
knn_train_accu = accuracy_score(y_train, knn_train_pred)
print(knn_train_accu)

cm=confusion_matrix(y_test,knn_predictions)
print("Confussion Matrix")
print(cm)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the confusion matrix
cm = confusion_matrix(y_test, knn_predictions)

# Plotting confusion matrix using seaborn
plt.figure(figsize=(3, 2))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Convert accuracies to percentage
dcl_accuracy_percentage = DecisionTree_accuracy * 100
logreg_accuracy_percentage = acc_log * 100
knn_accuracy_percentage = knn_accuracy * 100

# Create a bar chart
models = ['Decision Tree', 'Logistic Regression', 'k-Nearest Neighbors']
accuracies = [dcl_accuracy_percentage, logreg_accuracy_percentage, knn_accuracy_percentage]

plt.figure(figsize=(10, 5))
plt.bar(models, accuracies, color=['blue', 'green', 'orange'])
plt.title('Model Accuracies')
plt.xlabel('Models')
plt.ylabel('Accuracy (%)')
plt.ylim(0, 100)  # Set the y-axis limit from 0 to 100 for percentage
plt.show()

import joblib

model=dcl
joblib.dump(model,"decision_tree_pretrained.pkl")

from google.colab import files
files.download("decision_tree_pretrained.pkl")