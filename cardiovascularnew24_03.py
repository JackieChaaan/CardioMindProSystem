# -*- coding: utf-8 -*-
"""CardioVascularNew24/03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zvzw-aNJcPTNyZMhmnMQC7O4dejACgdi

#Sprint 1- Using Predefined Models

##Import Necessory Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


# preprocessing
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV

# models
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import ConfusionMatrixDisplay,classification_report,accuracy_score,confusion_matrix

"""##Load the dataset"""

#Dataset Reading
dataset=pd.read_csv('cardio_train.csv',sep=';')
dataset

dataset.shape

#Dataset information
dataset.info()

dataset.dtypes

#Check null values
dataset.isnull().sum()

#Check Duplicate Values
dataset.duplicated().sum()

#convert ages from days to years
dataset["age"]=(dataset["age"]/365).values.astype(int)
dataset.head()

dataset.describe()

dataset['cardio'].value_counts()

import pandas as pd

# Assuming 'df' is your DataFrame containing the dataset
# Replace 'categorical_vars' with the list of categorical variables in your dataset

categorical_vars = ['gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']

# Loop through each categorical variable and display its unique values
for var in categorical_vars:
    unique_values = dataset[var].unique()
    print(f"Unique values for {var}: {unique_values}")

import pandas as pd

# Assuming 'df' is your DataFrame containing the dataset
# Replace 'class_label' with the name of your target variable

class_label = 'cardio'

# Display unique values of the class label
unique_values = dataset[class_label].unique()
print(f"Unique values for {class_label}: {unique_values}")

"""##Visualization"""

dataset.hist(figsize=(15,12))

fig, ax = plt.subplots(1, 2, figsize=(20, 7))

sns.countplot(data=dataset, x="cardio", ax=ax[0])
dataset["cardio"].value_counts().plot.pie(explode=[0.1, 0],autopct="%1.1f%%", labels=["0", "1"], shadow=True, ax=ax[1])
fig.suptitle("Target Column distribution")

plt.show()

from matplotlib import rcParams
rcParams['figure.figsize'] = 14, 5
sns.countplot(x='age', hue='cardio', data = dataset, palette="Set2");

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

df_categorical = dataset.loc[:,['cholesterol', 'gluc', 'smoke', 'alco', 'active']]

# Set the figure size
plt.figure(figsize=(12, 5))

# Create the count plot
sns.countplot(x="variable", hue="value", data=pd.melt(df_categorical), palette="Set2")

plt.show()

df_long = pd.melt(dataset, id_vars=['cardio'], value_vars=['cholesterol','gluc', 'smoke', 'alco', 'active'])
sns.catplot(x="variable", hue="value", col="cardio",
                data=df_long, kind="count");

dataset.boxplot(figsize=(14,6))

dataset[["age","height","weight","ap_hi","ap_lo"]].boxplot()

"""##Dataset Preprocessing"""

#Remove Id
dataset.drop("id",axis=1,inplace=True)

import pandas as pd

# Calculate the IQR for ap_hi and ap_lo
Q1_hi = dataset['ap_hi'].quantile(0.25)
Q3_hi = dataset['ap_hi'].quantile(0.75)
IQR_hi = Q3_hi - Q1_hi

Q1_lo = dataset['ap_lo'].quantile(0.25)
Q3_lo = dataset['ap_lo'].quantile(0.75)
IQR_lo = Q3_lo - Q1_lo

# Define the upper and lower bounds for outliers
upper_bound_hi = Q3_hi + 1.5 * IQR_hi
lower_bound_hi = Q1_hi - 1.5 * IQR_hi

upper_bound_lo = Q3_lo + 1.5 * IQR_lo
lower_bound_lo = Q1_lo - 1.5 * IQR_lo

# Identify and filter out outliers
outliers_hi = (dataset['ap_hi'] > upper_bound_hi) | (dataset['ap_hi'] < lower_bound_hi)
outliers_lo = (dataset['ap_lo'] > upper_bound_lo) | (dataset['ap_lo'] < lower_bound_lo)

# Replace outliers with the median (you can choose a different strategy based on your analysis)
dataset.loc[outliers_hi, 'ap_hi'] = dataset['ap_hi'].median()
dataset.loc[outliers_lo, 'ap_lo'] = dataset['ap_lo'].median()
dataset.boxplot(figsize=(14,6))

# Calculate the IQR for height, and weight

Q1_height = dataset['height'].quantile(0.25)
Q3_height = dataset['height'].quantile(0.75)
IQR_height = Q3_height - Q1_height

Q1_weight = dataset['weight'].quantile(0.25)
Q3_weight = dataset['weight'].quantile(0.75)
IQR_weight = Q3_weight - Q1_weight

upper_bound_height = Q3_height + 1.5 * IQR_height
lower_bound_height = Q1_height - 1.5 * IQR_height

upper_bound_weight = Q3_weight + 1.5 * IQR_weight
lower_bound_weight = Q1_weight - 1.5 * IQR_weight

# Identify and filter out outliers for height, and weight
outliers_height = (dataset['height'] > upper_bound_height) | (dataset['height'] < lower_bound_height)
outliers_weight = (dataset['weight'] > upper_bound_weight) | (dataset['weight'] < lower_bound_weight)

# Replace outliers with the median (you can choose a different strategy based on your analysis)
dataset.loc[outliers_height, 'height'] = dataset['height'].median()
dataset.loc[outliers_weight, 'weight'] = dataset['weight'].median()

dataset.boxplot(figsize=(14,6))

dataset.hist(figsize=(15,12))

# Calculate the IQR for age, height, and weight
Q1_age = dataset['age'].quantile(0.25)
Q3_age = dataset['age'].quantile(0.75)
IQR_age = Q3_age - Q1_age

Q1_weight = dataset['weight'].quantile(0.25)
Q3_weight = dataset['weight'].quantile(0.75)
IQR_weight = Q3_weight - Q1_weight

# Define the upper and lower bounds for outliers
upper_bound_age = Q3_age + 1.5 * IQR_age
lower_bound_age = Q1_age - 1.5 * IQR_age

upper_bound_weight = Q3_weight + 1.5 * IQR_weight
lower_bound_weight = Q1_weight - 1.5 * IQR_weight

# Identify and filter out outliers for age and weight
outliers_age = (dataset['age'] > upper_bound_age) | (dataset['age'] < lower_bound_age)
outliers_weight = (dataset['weight'] > upper_bound_weight) | (dataset['weight'] < lower_bound_weight)

# Replace outliers with the median (you can choose a different strategy based on your analysis)
dataset.loc[outliers_age, 'age'] = dataset['age'].median()
dataset.loc[outliers_weight, 'weight'] = dataset['weight'].median()
dataset.boxplot(figsize=(14,6))

dataset["bmi"] = np.round(dataset.weight/(dataset.height/100)**2,2)
print(dataset.shape)
dataset.head(10)

dataset.boxplot(figsize=(14,6))

dataset.isnull().sum()

#Correlation Matrix
corr=dataset.corr().style.background_gradient(cmap='gray')
corr

dataset['gluc'].unique()

#Let's visualize the correlation matrix iin the seaborn
sns.set(rc={'figure.figsize':(15,7)})
sns.heatmap(dataset.corr(),annot=True,cmap='rainbow')

import pandas as pd
from imblearn.over_sampling import RandomOverSampler

# Assuming 'dataset' is your cardiovascular disease dataset DataFrame
# Separate features (x) and target variable (y)
xx = dataset.drop('gender', axis=1)
yy = dataset['gender']

# Determine the desired number of samples for each class in the 'gender' column
desired_samples = yy.value_counts().max()  # Adjust this as needed

# Define the sampling strategy for RandomOverSampler
sampling_strategy = {1: desired_samples, 2: desired_samples}

# Apply random oversampling to the entire dataset for the 'gender' column
ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)
X_resampled, y_resampled = ros.fit_resample(xx, yy)

# Combine the resampled data back into a DataFrame
dataset = pd.concat([pd.DataFrame(X_resampled, columns=xx.columns), pd.DataFrame(y_resampled, columns=['gender'])], axis=1)

# Display the resampled 'gender' column counts
print(dataset['gender'].value_counts())

dataset.boxplot(figsize=(14,6))

dataset = dataset[['age', 'gender', 'height', 'weight', 'bmi', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio']]

"""##Split the dataset"""

x=dataset.drop('cardio',axis=1)
y=dataset['cardio']
x_train,x_test,y_train,y_test=train_test_split(x, y,test_size=0.2,random_state=42)

x_train

y_train

x_test=pd.DataFrame(x_test, columns=x.columns)
print(x_test.head(50))

print(y_test.head(50))

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

"""##Model Training and prediction

###Model 1 - Decision tree
"""

#Train decision tree
dcl=DecisionTreeClassifier()
dcl.fit(x_train,y_train)

#Train accuracy
train_predict = dcl.predict(x_train)
dcl_train_accura = accuracy_score(y_train,train_predict)


print(f"Train Accuracy:{dcl_train_accura:.2f}")

#Test accuracy
predict=dcl.predict(x_test)
DecisionTree_accuracy=accuracy_score(y_test,predict)
cm1=confusion_matrix(y_test,predict)
print(f"Test Accuracy:{DecisionTree_accuracy:.2f}")
print("\nDecision Tree Classification Report:\n", classification_report(y_test, predict))
print("Confusion Matxir",cm1)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, predict)

# Plotting confusion matrix using seaborn
plt.figure(figsize=(3, 2))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# # Assuming you have a trained Decision Tree model (dcl)

# # Get user input for features
# age = float(input("Enter age: "))
# gender = int(input("Enter gender (1 for male, 2 for female): "))
# height = int(input("Enter height in cm: "))
# weight = float(input("Enter weight in kg: "))
# bmi = float(input("Enter BMI: "))
# ap_hi = int(input("Enter systolic blood pressure (ap_hi): "))
# ap_lo = int(input("Enter diastolic blood pressure (ap_lo): "))
# cholesterol = int(input("Enter cholesterol level: "))
# gluc = int(input("Enter glucose level: "))
# smoke = int(input("Enter smoking status (0 for non-smoker, 1 for smoker): "))
# alco = int(input("Enter alcohol consumption status (0 for non-drinker, 1 for drinker): "))
# active = int(input("Enter physical activity status (0 for inactive, 1 for active): "))

# # Create a DataFrame with user input
# new_data = pd.DataFrame({
#     'age': [age],
#     'gender': [gender],
#     'height': [height],
#     'weight': [weight],
#     'bmi': [bmi],
#     'ap_hi': [ap_hi],
#     'ap_lo': [ap_lo],
#     'cholesterol': [cholesterol],
#     'gluc': [gluc],
#     'smoke': [smoke],
#     'alco': [alco],
#     'active': [active]
# })

# new_data_scaled = scaler.transform(new_data)
# # Make predictions using the Decision Tree model (dcl)
# predictions = dcl.predict(new_data_scaled)

# # Display the predictions
# print("Predictions:", predictions[0])

"""###Improve the accuracy-Hyper paramaeter tuning using grid search"""

# from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix
from sklearn.model_selection import GridSearchCV
import joblib

# Train the initial decision tree model
dcl = DecisionTreeClassifier()
dcl.fit(x_train, y_train)

# Evaluate initial model
train_predict = dcl.predict(x_train)
dcl_train_accuracy = accuracy_score(y_train, train_predict)
predict = dcl.predict(x_test)
DecisionTree_accuracy = accuracy_score(y_test, predict)

print(f"Initial Train Accuracy: {dcl_train_accuracy:.2f}")
print(f"Initial Test Accuracy: {DecisionTree_accuracy:.2f}")
print("\nInitial Decision Tree Classification Report:\n", classification_report(y_test, predict))

# Define the parameter grid to search
param_grid_dcl = {
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt']  # Set to 'sqrt' to resolve deprecation warning
}

# Perform Grid Search
grid_search_dcl = GridSearchCV(DecisionTreeClassifier(), param_grid_dcl, cv=5, scoring='accuracy')
grid_search_dcl.fit(x_train, y_train)

# Get the best parameters and best estimator
best_params_dcl = grid_search_dcl.best_params_
best_estimator_dcl = grid_search_dcl.best_estimator_

# Use the best estimator for prediction
predictions = best_estimator_dcl.predict(x_test)
DecisionTreeModel_accuracy = accuracy_score(y_test, predictions)

print("\nBest Parameters:", best_params_dcl)
print("Test Accuracy after Grid Search:", DecisionTreeModel_accuracy)
cm=confusion_matrix(y_test,predictions)
print("Confusion matrix",cm)

# # Save the best model
# # joblib.dump(best_estimator, 'best_decision_tree_model.pkl')
# # print("Best model saved successfully!")

# Calculate the confusion matrix
cm=confusion_matrix(y_test,predictions)

# Plotting confusion matrix using seaborn
plt.figure(figsize=(3, 2))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

print("\nInitial Decision Tree Classification Report:\n", classification_report(y_test, predictions))

# Access feature importances from the best_estimator
feature_importances = best_estimator_dcl.feature_importances_

# Create a DataFrame to display feature importanc_dcles
importance_df = pd.DataFrame({
    'Feature': x.columns,
    'Importance': feature_importances
})

# Sort the DataFrame by importance score in descending order
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Display the feature importances
print(importance_df)

"""####Decision Tree from scratch"""

import numpy as np

class DecisionTreeClassifier:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth

    def fit(self, X, y):
        self.tree = self._grow_tree(X, y)

    def _grow_tree(self, X, y, depth=0):
        num_samples, num_features = X.shape
        num_samples_per_class = [np.sum(y == c) for c in np.unique(y)]
        predicted_class = np.argmax(num_samples_per_class)

        # Stopping criteria
        if (depth == self.max_depth) or (len(np.unique(y)) == 1):
            return {'class': predicted_class, 'num_samples': num_samples}

        # Find best split
        best_gini = float('inf')
        best_feature = None
        best_threshold = None
        for feature in range(num_features):
            thresholds = np.unique(X[:, feature])
            for threshold in thresholds:
                left_indices = X[:, feature] <= threshold
                right_indices = X[:, feature] > threshold

                left_gini = self._gini(y[left_indices])
                right_gini = self._gini(y[right_indices])

                gini = (left_gini * np.sum(left_indices) + right_gini * np.sum(right_indices)) / num_samples
                if gini < best_gini:
                    best_gini = gini
                    best_feature = feature
                    best_threshold = threshold

        left_indices = X[:, best_feature] <= best_threshold
        right_indices = X[:, best_feature] > best_threshold

        left_subtree = self._grow_tree(X[left_indices], y[left_indices], depth + 1)
        right_subtree = self._grow_tree(X[right_indices], y[right_indices], depth + 1)

        return {'feature': best_feature, 'threshold': best_threshold,
                'left': left_subtree, 'right': right_subtree}

    def _gini(self, y):
        num_samples = len(y)
        _, counts = np.unique(y, return_counts=True)
        probs = counts / num_samples
        return 1 - np.sum(probs ** 2)

    def predict(self, X):
        return [self._predict_tree(x, self.tree) for x in X]

    def _predict_tree(self, x, tree):
        if 'class' in tree:
            return tree['class']
        if x[tree['feature']] <= tree['threshold']:
            return self._predict_tree(x, tree['left'])
        else:
            return self._predict_tree(x, tree['right'])

# Example usage:
# Instantiate and train the decision tree classifier
dcl_scratch = DecisionTreeClassifier(max_depth=5)
dcl_scratch .fit(x_train, y_train)

# Make predictions on the test set
predictions = dcl_scratch .predict(x_test)

# Evaluate the model
dcl_scratch_accuracy = accuracy_score(y_test, predictions)
conf_matrix = confusion_matrix(y_test, predictions)
print("Test Accuracy:", dcl_scratch_accuracy)
print("Confusion matrix:", conf_matrix)

plt.figure(figsize=(3, 2))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

print("\nDecision Tree from scratch Classification Report:\n", classification_report(y_test, predictions))

"""###User Data"""

# # Assuming you have a trained Decision Tree model (dcl)

# # Get user input for features
# age = float(input("Enter age: "))
# gender = int(input("Enter gender (1 for male, 2 for female): "))
# height = int(input("Enter height in cm: "))
# weight = float(input("Enter weight in kg: "))
# bmi = float(input("Enter BMI: "))
# ap_hi = int(input("Enter systolic blood pressure (ap_hi): "))
# ap_lo = int(input("Enter diastolic blood pressure (ap_lo): "))
# cholesterol = int(input("Enter cholesterol level: "))
# gluc = int(input("Enter glucose level: "))
# smoke = int(input("Enter smoking status (0 for non-smoker, 1 for smoker): "))
# alco = int(input("Enter alcohol consumption status (0 for non-drinker, 1 for drinker): "))
# active = int(input("Enter physical activity status (0 for inactive, 1 for active): "))

# # Create a DataFrame with user input
# new_data_dcl = pd.DataFrame({
#     'age': [age],
#     'gender': [gender],
#     'height': [height],
#     'weight': [weight],
#     'bmi': [bmi],
#     'ap_hi': [ap_hi],
#     'ap_lo': [ap_lo],
#     'cholesterol': [cholesterol],
#     'gluc': [gluc],
#     'smoke': [smoke],
#     'alco': [alco],
#     'active': [active]
# })

# new_data_scaled_dcl = scaler.transform(new_data_dcl)
# # Make predictions using the Decision Tree model (dcl)
# predictions = best_estimator_dcl.predict(new_data_scaled_dcl)

# # Display the predictions
# print("Predictions:", predictions[0])

# import pickle

# # Assuming dt_scratch is your trained decision tree model

# # Save the model to a file
# with open('decisionTree_pretrained25-03-24Scaler.pkl', 'wb') as file:
#     pickle.dump(scaler, file)

# import pickle

# # Assuming dt_scratch is your trained decision tree model

# # Save the model to a file
# with open('decisionTree_pretrained25-03-24.pkl', 'wb') as file:
#     pickle.dump(best_estimator, file)

# from sklearn.ensemble import RandomForestClassifier

# rf=RandomForestClassifier()
# rf.fit(x_train,y_train)
# pred=rf.predict(x_test)
# accu=accuracy_score(y_test,pred)
# print("Accuracy",accu)

# # Access feature importances from the best_estimator
# feature_importances = rf.feature_importances_

# # Create a DataFrame to display feature importances
# importance_df = pd.DataFrame({
#     'Feature': x.columns,
#     'Importance': feature_importances
# })

# # Sort the DataFrame by importance score in descending order
# importance_df = importance_df.sort_values(by='Importance', ascending=False)

# # Display the feature importances
# print(importance_df)

"""###Model 2 - KNN"""

from sklearn.neighbors import KNeighborsClassifier

knn_model=KNeighborsClassifier()
knn_model.fit(x_train,y_train)

#Train accuracy
train_predict = knn_model.predict(x_train)
knn_train_accura = accuracy_score(y_train,train_predict)


print(f"Train Accuracy:{knn_train_accura:.2f}")

#Test accuracy
predict=knn_model.predict(x_test)
knn_accuracy=accuracy_score(y_test,predict)
cm1=confusion_matrix(y_test,predict)


print(f"Test Accuracy:{knn_accuracy:.2f}")
print("\nKNN Classification Report:\n", classification_report(y_test, predict))
print("Confusion Matxir",cm1)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, predict)

# Plotting confusion matrix using seaborn
plt.figure(figsize=(3, 2))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# # Assuming you have a trained Decision Tree model (dcl)

# # Get user input for features
# age = float(input("Enter age: "))
# gender = int(input("Enter gender (1 for male, 2 for female): "))
# height = int(input("Enter height in cm: "))
# weight = float(input("Enter weight in kg: "))
# bmi = float(input("Enter BMI: "))
# ap_hi = int(input("Enter systolic blood pressure (ap_hi): "))
# ap_lo = int(input("Enter diastolic blood pressure (ap_lo): "))
# cholesterol = int(input("Enter cholesterol level: "))
# gluc = int(input("Enter glucose level: "))
# smoke = int(input("Enter smoking status (0 for non-smoker, 1 for smoker): "))
# alco = int(input("Enter alcohol consumption status (0 for non-drinker, 1 for drinker): "))
# active = int(input("Enter physical activity status (0 for inactive, 1 for active): "))

# # Create a DataFrame with user input
# new_data_knn = pd.DataFrame({
#     'age': [age],
#     'gender': [gender],
#     'height': [height],
#     'weight': [weight],
#     'bmi': [bmi],
#     'ap_hi': [ap_hi],
#     'ap_lo': [ap_lo],
#     'cholesterol': [cholesterol],
#     'gluc': [gluc],
#     'smoke': [smoke],
#     'alco': [alco],
#     'active': [active]
# })

# new_data_scaled_knn = scaler.transform(new_data_knn)
# # Make predictions using the Decision Tree model (dcl)
# predictions_knn = knn_model.predict(new_data_scaled_knn)

# # Display the predictions
# print("Predictions:", predictions_knn[0])

# import pickle

# # Assuming dt_scratch is your trained decision tree model

# # Save the model to a file
# with open('knn_pretrained25-03-24Scaler.pkl', 'wb') as file:
#     pickle.dump(scaler, file)

# import pickle

# # Assuming dt_scratch is your trained decision tree model

# # Save the model to a file
# with open('KNN_pretrained_newupdate25-03-24.pkl', 'wb') as file:
#     pickle.dump(knn_model, file)

# from sklearn.model_selection import GridSearchCV
# # from sklearn.neighbors import KNeighborsClassifier
# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# # Define the parameter grid for GridSearchCV
# param_grid_knn = {
#     'n_neighbors': [3, 5, 7, 9],  # Adjust values as needed
#     'weights': ['uniform', 'distance'],  # Try different weight options
#     'metric': ['euclidean', 'manhattan']  # Try different distance metrics
# }

# # Create the KNN classifier
# knn = KNeighborsClassifier()

# # Initialize GridSearchCV
# grid_search_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='accuracy')

# # Fit GridSearchCV to the training data
# grid_search_knn.fit(x_train, y_train)

# # Get the best parameters and best estimator from GridSearchCV
# best_params_knn = grid_search_knn.best_params_
# best_estimator_knn = grid_search_knn.best_estimator_

# # Use the best estimator for prediction
# predictions_train_knn = best_estimator_knn.predict(x_train)
# predictions_test_knn = best_estimator_knn.predict(x_test)

# # Calculate accuracy scores
# train_accuracy = accuracy_score(y_train, predictions_train_knn)
# test_accuracy = accuracy_score(y_test, predictions_test_knn)

# # Calculate confusion matrix
# cm_train = confusion_matrix(y_train, predictions_train_knn)
# cm_test = confusion_matrix(y_test, predictions_test_knn)

# # Print results
# print("Best Parameters:", best_params_knn)
# print("Train Accuracy:", train_accuracy)
# print("Test Accuracy:", test_accuracy)
# print("\nTrain Confusion Matrix:\n", cm_train)
# print("\nTest Confusion Matrix:\n", cm_test)

# # Assuming you have a trained Decision Tree model (dcl)

# # Get user input for features
# age = float(input("Enter age: "))
# gender = int(input("Enter gender (1 for male, 2 for female): "))
# height = int(input("Enter height in cm: "))
# weight = float(input("Enter weight in kg: "))
# bmi = float(input("Enter BMI: "))
# ap_hi = int(input("Enter systolic blood pressure (ap_hi): "))
# ap_lo = int(input("Enter diastolic blood pressure (ap_lo): "))
# cholesterol = int(input("Enter cholesterol level: "))
# gluc = int(input("Enter glucose level: "))
# smoke = int(input("Enter smoking status (0 for non-smoker, 1 for smoker): "))
# alco = int(input("Enter alcohol consumption status (0 for non-drinker, 1 for drinker): "))
# active = int(input("Enter physical activity status (0 for inactive, 1 for active): "))

# # Create a DataFrame with user input
# new_data_knn = pd.DataFrame({
#     'age': [age],
#     'gender': [gender],
#     'height': [height],
#     'weight': [weight],
#     'bmi': [bmi],
#     'ap_hi': [ap_hi],
#     'ap_lo': [ap_lo],
#     'cholesterol': [cholesterol],
#     'gluc': [gluc],
#     'smoke': [smoke],
#     'alco': [alco],
#     'active': [active]
# })

# new_data_scaled_knn = scaler.transform(new_data_knn)
# # Make predictions using the Decision Tree model (dcl)
# predictions_knn = best_estimator_knn.predict(new_data_scaled_knn)

# # Display the predictions
# print("Predictions:", predictions_knn[0])

# import pickle

# # Assuming dt_scratch is your trained decision tree model

# # Save the model to a file
# with open('KNN_pretrained25-03-24.pkl', 'wb') as file:
#     pickle.dump(knn, file)

# #Feature Importance
# from sklearn.inspection import permutation_importance

# # Assuming you have a trained KNN model named knn_model
# # Perform permutation feature importance
# result = permutation_importance(knn, x_test, y_test, n_repeats=10, random_state=42)

# # Get feature importances
# importance_df = pd.DataFrame({
#     'Feature': x.columns,
#     'Importance': result.importances_mean
# })

# # Sort the DataFrame by importance score in descending order
# importance_df = importance_df.sort_values(by='Importance', ascending=False)

# # Display the feature importances
# print(importance_df)

"""###Model 3 - Logistic Regression"""

from sklearn.linear_model import LogisticRegression

lr=LogisticRegression()
lr.fit(x_train,y_train)

#Train accuracy
train_predict = lr.predict(x_train)
lr_train_accura = accuracy_score(y_train,train_predict)


print(f"Train Accuracy:{lr_train_accura:.2f}")

#Test accuracy
predict=lr.predict(x_test)
lr_accuracy=accuracy_score(y_test,predict)
cm1=confusion_matrix(y_test,predict)


print(f"Test Accuracy:{lr_accuracy:.2f}")
print("\nLogistic Regression Classification Report:\n", classification_report(y_test, predict))
print("Confusion Matxir",cm1)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, predict)

# Plotting confusion matrix using seaborn
plt.figure(figsize=(3, 2))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# import pickle

# # Assuming dt_scratch is your trained decision tree model

# # Save the model to a file
# with open('logreg_pretrained25-03-24Scaler.pkl', 'wb') as file:
#     pickle.dump(scaler, file)

# import pickle

# # Assuming dt_scratch is your trained decision tree model

# # Save the model to a file
# with open('logreg_pretrained_new25-03-24.pkl', 'wb') as file:
#     pickle.dump(lr, file)

# # Assuming you have a trained Decision Tree model (dcl)

# # Get user input for features
# age = float(input("Enter age: "))
# gender = int(input("Enter gender (1 for male, 2 for female): "))
# height = int(input("Enter height in cm: "))
# weight = float(input("Enter weight in kg: "))
# bmi = float(input("Enter BMI: "))
# ap_hi = int(input("Enter systolic blood pressure (ap_hi): "))
# ap_lo = int(input("Enter diastolic blood pressure (ap_lo): "))
# cholesterol = int(input("Enter cholesterol level: "))
# gluc = int(input("Enter glucose level: "))
# smoke = int(input("Enter smoking status (0 for non-smoker, 1 for smoker): "))
# alco = int(input("Enter alcohol consumption status (0 for non-drinker, 1 for drinker): "))
# active = int(input("Enter physical activity status (0 for inactive, 1 for active): "))

# # Create a DataFrame with user input
# new_data_lr = pd.DataFrame({
#     'age': [age],
#     'gender': [gender],
#     'height': [height],
#     'weight': [weight],
#     'bmi': [bmi],
#     'ap_hi': [ap_hi],
#     'ap_lo': [ap_lo],
#     'cholesterol': [cholesterol],
#     'gluc': [gluc],
#     'smoke': [smoke],
#     'alco': [alco],
#     'active': [active]
# })

# new_data_scaled_lr = scaler.transform(new_data_lr)
# # Make predictions using the Decision Tree model (dcl)
# predictions_lr = lr.predict(new_data_scaled_lr)

# # Display the predictions
# print("Predictions:", predictions_lr[0])

# #Feature Importance
# # Access coefficients from the best_estimator (assuming logreg is your logistic regression model)
# coefficients = lr.coef_[0]

# # Create a DataFrame to display feature importances
# importance_df = pd.DataFrame({
#     'Feature': x.columns,
#     'Coefficient': coefficients
# })

# # Sort the DataFrame by coefficient magnitude
# importance_df['Absolute Coefficient'] = np.abs(importance_df['Coefficient'])
# importance_df = importance_df.sort_values(by='Absolute Coefficient', ascending=False)

# # Display the feature importances
# print(importance_df)

# from sklearn.metrics import accuracy_score, confusion_matrix

# # Define the grid of hyperparameters to search
# param_grid_lr = {
#     'penalty': ['l1', 'l2'],                # Regularization penalty
#     'C': [0.001, 0.01, 0.1, 1, 10, 100],    # Inverse of regularization strength
#     'solver': ['liblinear', 'saga'],        # Solver for optimization problem
#     'max_iter': [100, 200, 300]             # Maximum number of iterations
# }

# # Create a grid search object
# grid_search_lr = GridSearchCV(lr, param_grid_lr, cv=5, scoring='accuracy')

# # Perform grid search on the training data
# grid_search_lr.fit(x_train, y_train)

# # Get the best hyperparameters and best estimator
# best_params_lr = grid_search_lr.best_params_
# best_estimator_lr = grid_search_lr.best_estimator_

# # Use the best estimator for prediction
# predictions_lr = best_estimator_lr.predict(x_test)

# # Evaluate the model
# test_accuracy = accuracy_score(y_test, predictions_lr)
# train_accuracy = accuracy_score(y_train, best_estimator_lr.predict(x_train))
# conf_matrix = confusion_matrix(y_test, predictions_lr)

# # Display results
# print("Best Parameters:", best_params_lr)
# print("Train Accuracy after Grid Search:", train_accuracy)
# print("Test Accuracy after Grid Search:", test_accuracy)
# print("\nConfusion Matrix:")
# print(conf_matrix)

# # Assuming you have a trained Decision Tree model (dcl)

# # Get user input for features
# age = float(input("Enter age: "))
# gender = int(input("Enter gender (1 for male, 2 for female): "))
# height = int(input("Enter height in cm: "))
# weight = float(input("Enter weight in kg: "))
# bmi = float(input("Enter BMI: "))
# ap_hi = int(input("Enter systolic blood pressure (ap_hi): "))
# ap_lo = int(input("Enter diastolic blood pressure (ap_lo): "))
# cholesterol = int(input("Enter cholesterol level: "))
# gluc = int(input("Enter glucose level: "))
# smoke = int(input("Enter smoking status (0 for non-smoker, 1 for smoker): "))
# alco = int(input("Enter alcohol consumption status (0 for non-drinker, 1 for drinker): "))
# active = int(input("Enter physical activity status (0 for inactive, 1 for active): "))

# # Create a DataFrame with user input
# new_data_lr = pd.DataFrame({
#     'age': [age],
#     'gender': [gender],
#     'height': [height],
#     'weight': [weight],
#     'bmi': [bmi],
#     'ap_hi': [ap_hi],
#     'ap_lo': [ap_lo],
#     'cholesterol': [cholesterol],
#     'gluc': [gluc],
#     'smoke': [smoke],
#     'alco': [alco],
#     'active': [active]
# })

# new_data_scaled_lr = scaler.transform(new_data_lr)
# # Make predictions using the Decision Tree model (dcl)
# predictions_lr = best_estimator_lr.predict(new_data_scaled_lr)

# # Display the predictions
# print("Predictions:", predictions_lr[0])

# import pickle

# # Assuming dt_scratch is your trained decision tree model

# # Save the model to a file
# with open('Logistc_Regression_pretrained25-03-24.pkl', 'wb') as file:
#     pickle.dump(lr, file)

"""##Save Model"""

# import pickle

# # Assuming dt_scratch is your trained decision tree model

# # Save the model to a file
# with open('logistic_regressionNew2103.pkl', 'wb') as file:
#     pickle.dump(logreg, file)

"""#Accuracy Comparison of 3 Model and scratch"""

# Convert accuracies to percentage
dcl_accuracy_percentage = DecisionTreeModel_accuracy * 100
logreg_accuracy_percentage = lr_accuracy * 100
knn_accuracy_percentage = knn_accuracy * 100
decl_scratch_accu_percent = dcl_scratch_accuracy * 100

# Create a bar chart
models = ['Decision Tree', 'Logistic Regression', 'k-Nearest Neighbors', 'Decision Tree From Scratch']
accuracies = [dcl_accuracy_percentage, logreg_accuracy_percentage, knn_accuracy_percentage, decl_scratch_accu_percent]

plt.figure(figsize=(10, 5))
plt.bar(models, accuracies, color=['blue', 'green', 'orange', 'red'])
plt.title('Model Accuracies')
plt.xlabel('Models')
plt.ylabel('Accuracy (%)')
plt.ylim(0, 100)  # Set the y-axis limit from 0 to 100 for percentage
plt.show()

# Create a bar chart
models = ['Decision Tree', 'Logistic Regression', 'k-Nearest Neighbors', 'Decision Tree From Scratch']
accuracies = [dcl_accuracy_percentage, logreg_accuracy_percentage, knn_accuracy_percentage, decl_scratch_accu_percent]

plt.figure(figsize=(10, 5))
bars = plt.bar(models, accuracies, color=['blue', 'green', 'orange', 'red'])
plt.title('Model Accuracies')
plt.xlabel('Models')
plt.ylabel('Accuracy (%)')
plt.ylim(0, 100)  # Set the y-axis limit from 0 to 100 for percentage

# Add accuracy values above the bars
for bar, accuracy in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 2, f'{accuracy:.2f}%', ha='center', va='bottom')

plt.show()